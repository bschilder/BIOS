{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eaf3855e",
      "metadata": {},
      "source": [
        "# Build Your Own Transformer for Text Generation\n",
        "\n",
        "This notebook shows how to build a **tiny GPT‑style transformer** for text generation, step by step.\n",
        "It is written for someone who is **comfortable with code and math**, but **new to deep learning / transformers**.\n",
        "\n",
        "We will think of a transformer as a **smart autocomplete**:\n",
        "- You give it some text (a *prompt*),\n",
        "- It repeatedly guesses the **next character**,\n",
        "- Those guesses form new text.\n",
        "\n",
        "To do that, we will build the following pieces ourselves:\n",
        "\n",
        "1. **Token & position representation** – turn characters into vectors and tell the model *where* each token is\n",
        "2. **Causal self‑attention** – each position decides *which previous positions to pay attention to*\n",
        "3. **Multi‑head attention** – several attention \"views\" in parallel, then combined\n",
        "4. **Feed‑forward block** – a small per‑position MLP that helps mix information\n",
        "5. **Decoder blocks** – attention + feed‑forward + residual connections + layer norm\n",
        "6. **Training loop** – make the model good at “next‑token prediction” on a small text corpus\n",
        "7. **Generation loop** – use the model to sample new text one token at a time\n",
        "\n",
        "You can read this notebook top‑to‑bottom like a tutorial, or run it cell by cell as a lab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2767f91c",
      "metadata": {},
      "source": [
        "## 1. Imports and device\n",
        "\n",
        "We use **PyTorch** for tensors and neural network layers.\n",
        "\n",
        "The only slightly fancy thing we do here is pick a **device**:\n",
        "- If you have a GPU (CUDA or Apple Silicon MPS), we use it.\n",
        "- Otherwise we quietly fall back to CPU.\n",
        "\n",
        "Everything else in the notebook just uses this `device` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "160e04ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Pick the best available compute device.\n",
        "# Think of this as choosing whether to run on:\n",
        "# - a GPU in your laptop,\n",
        "# - or CPU if no GPU is available.\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")  # Apple Silicon GPU\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ffd8bda",
      "metadata": {},
      "source": [
        "## 2. Positional encoding\n",
        "\n",
        "A transformer looks at a **set of vectors**; by default it does not know which token came first, second, etc.\n",
        "\n",
        "To fix that, we *add* a vector that describes the **position** of each token:\n",
        "- Token at position 0 gets one vector,\n",
        "- Token at position 1 gets a different vector,\n",
        "- and so on.\n",
        "\n",
        "Here we use the classic **sinusoidal positional encoding** from the original transformer paper.\n",
        "You can think of it as giving each position a unique \"barcode\" of sines and cosines that the model can learn to interpret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d2173391",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Add a fixed \"position barcode\" to each token embedding.\n",
        "\n",
        "    Analogy: if each word is a *card* with some information,\n",
        "    positional encoding writes the *position number* on the card,\n",
        "    so the model can tell \"this was the 3rd word\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # pe has shape (max_len, d_model).\n",
        "        # Row i contains the positional encoding for position i.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
        "\n",
        "        # Different frequencies along the feature dimension.\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        # Even indices: sine, odd indices: cosine.\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension: (1, max_len, d_model).\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # register_buffer = tensor is part of the module, but not a learnable parameter.\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Add positional encodings to a batch of embeddings.\n",
        "\n",
        "        Args:\n",
        "            x: (batch, seq_len, d_model) token embeddings.\n",
        "        \"\"\"\n",
        "        # Slice `pe` to the sequence length we actually have, then add.\n",
        "        x = x + self.pe[:, : x.size(1), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Causal self-attention\n",
        "\n",
        "For **text generation**, we must prevent the model from \"seeing\" future tokens. We use a **causal mask**: position $i$ can only attend to positions $\\leq i$. We implement scaled dot-product attention with a lower-triangular mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def causal_attention_scores(q, k, mask_value=-1e9):\n",
        "    \"\"\"\n",
        "    Scaled dot-product attention with causal mask.\n",
        "\n",
        "    Args:\n",
        "        q (torch.Tensor): Query tensor of shape (batch, heads, seq_len, d_k).\n",
        "        k (torch.Tensor): Key tensor of shape (batch, heads, seq_len, d_k).\n",
        "        mask_value (float, optional): Value to use for masked (future) positions above the diagonal. Default: -1e9.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Attention scores of shape (batch, heads, seq_len, seq_len), with masked (upper-triangular, i.e. future) positions set to mask_value.\n",
        "    \"\"\"\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    seq_len = scores.size(-1)\n",
        "    causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=scores.device), diagonal=1).bool()\n",
        "    scores = scores.masked_fill(causal_mask, mask_value)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Multi-head attention\n",
        "\n",
        "We run several **attention heads** in parallel, then concatenate and project. Each head has its own Q, K, V projections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CausalMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        batch, seq_len, _ = x.shape\n",
        "        q = self.w_q(x).view(batch, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.w_k(x).view(batch, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.w_v(x).view(batch, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        scores = causal_attention_scores(q, k)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch, seq_len, self.d_model)\n",
        "        return self.w_o(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feed-forward block\n",
        "\n",
        "Each transformer block has a two-layer MLP after attention: linear → ReLU → linear, often with an inner dimension 4× the model size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int = None, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        d_ff = d_ff or 4 * d_model\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Decoder block\n",
        "\n",
        "One block = **causal multi-head attention** (with residual + layer norm) followed by **feed-forward** (with residual + layer norm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, d_ff: int = None, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = CausalMultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout(self.self_attn(self.norm1(x)))\n",
        "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Full decoder-only transformer\n",
        "\n",
        "We stack decoder blocks on top of **token embeddings + positional encoding**, then project the last hidden state to the vocabulary size for next-token prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerLM(nn.Module):\n",
        "    \"\"\"Decoder-only language model for next-token prediction.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 128,\n",
        "        num_heads: int = 4,\n",
        "        num_layers: int = 3,\n",
        "        d_ff: int = None,\n",
        "        max_len: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = PositionalEncoding(d_model, max_len, dropout)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DecoderBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len) token ids\n",
        "        x = self.embed(x)\n",
        "        x = self.pos(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.norm(x)\n",
        "        logits = self.head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Character-level tokenizer\n",
        "\n",
        "To keep the tutorial self-contained we use a **character-level** vocabulary: each character is a token. You can replace this with a subword tokenizer (e.g. BPE) later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CharTokenizer:\n",
        "    def __init__(self, text: str):\n",
        "        self.chars = sorted(set(text))\n",
        "        self.stoi = {c: i for i, c in enumerate(self.chars)}\n",
        "        self.itos = {i: c for c, i in self.stoi.items()}\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "    def encode(self, s: str):\n",
        "        return [self.stoi[c] for c in s]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return \"\".join(self.itos[i] for i in ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training loop\n",
        "\n",
        "We train on **next-token prediction**: given a sequence, predict the next token at each position. Loss is cross-entropy over the logits (shifted so targets are one position ahead)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch)\n",
        "        # Next-token prediction: predict batch[:, 1:] from logits[:, :-1]\n",
        "        logits_flat = logits[:, :-1].reshape(-1, logits.size(-1))\n",
        "        targets_flat = batch[:, 1:].reshape(-1)\n",
        "        loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Data and training run\n",
        "\n",
        "We create a small corpus and train for a few epochs. Inputs are random chunks of the text; each batch is (batch_size, block_size)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batches(data, batch_size, block_size):\n",
        "    \"\"\"Yield random (batch_size, block_size) chunks from data (1D tensor of token ids).\"\"\"\n",
        "    n = len(data)\n",
        "    for _ in range(n // (batch_size * block_size)):\n",
        "        starts = torch.randint(0, n - block_size, (batch_size,))\n",
        "        batch = torch.stack([data[s : s + block_size] for s in starts])\n",
        "        yield batch\n",
        "\n",
        "block_size = 32\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "lr = 3e-4\n",
        "\n",
        "text = \"\"\"\n",
        "The transformer is a deep learning architecture introduced in the paper Attention is All You Need.\n",
        "It relies entirely on self-attention for sequence modeling. We build a small one here for fun.\n",
        "\"\"\" * 20\n",
        "\n",
        "tokenizer = CharTokenizer(text)\n",
        "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
        "loader = list(get_batches(data, batch_size, block_size))\n",
        "\n",
        "model = TransformerLM(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    num_layers=3,\n",
        "    max_len=block_size,\n",
        "    dropout=0.1,\n",
        ").to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss = train_epoch(model, loader, optimizer, device)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}  loss = {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Text generation\n",
        "\n",
        "We **sample** one token at a time: feed the current context, take logits for the last position, sample from the probability distribution (optionally with temperature), append to context, repeat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, tokenizer, prompt: str, max_new_tokens: int = 80, temperature: float = 0.8):\n",
        "    model.eval()\n",
        "    ids = tokenizer.encode(prompt)\n",
        "    context = torch.tensor([ids], dtype=torch.long, device=device)\n",
        "    for _ in range(max_new_tokens):\n",
        "        if context.size(1) > block_size:\n",
        "            context = context[:, -block_size:]\n",
        "        logits = model(context)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        context = torch.cat([context, next_id], dim=1)\n",
        "    return tokenizer.decode(context[0].tolist())\n",
        "\n",
        "print(generate(model, tokenizer, \"The \", max_new_tokens=120, temperature=0.7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- **Larger model**: increase `d_model`, `num_heads`, `num_layers`, and train longer.\n",
        "- **Subword tokenizer**: use a BPE tokenizer (e.g. `tiktoken` or Hugging Face `tokenizers`) for word-level coherence.\n",
        "- **Larger data**: train on a real corpus (e.g. Wikipedia, books) with proper train/val split and batching.\n",
        "- **Learning rate schedule**: use cosine decay or warmup for better convergence."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm_topics",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
